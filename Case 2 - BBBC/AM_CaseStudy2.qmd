---
title: "CaseStudy2"
format: html
editor: visual
---

## libraies
```{r}
library(tidyverse)
library(cluster) 
library(factoextra) 
library(dendextend)
library(readxl)
library(dplyr)
library(car)
library(ggplot2)
```


## data
```{r}
getwd() #open R project file to have the same file path
train <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-train.xlsx")
test <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-test.xlsx")
```
### Exploring
```{r}
head(train)
str(train)

hist(train$Amount_purchased)
hist(train$Frequency)
hist(train$Last_purchase)
hist(train$First_purchase)
hist(train$P_Youth)
hist(train$P_DIY)
```
### logistic model
```{r}
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
glm1 <- glm(Choice ~ .-Observation, data = train)
summary(glm1)
vif(glm1)

glm1 <- glm(Choice ~ .-Observation-Last_purchase, data = train)
summary(glm1)
vif(glm1)

glm1 <- glm(Choice ~ .-Observation-Last_purchase-First_purchase, data = train)
summary(glm1)
vif(glm1)


#REMOVING Y_YOUTH AS ITS P-VALUE IS > 0.05
glm1<- glm(Choice ~  Gender + Amount_purchased + 
    Frequency +  + P_Child  + 
    P_Cook + P_DIY + P_Art
            , data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmTest_FINAL)
vif(glmTest_FINAL)



#REMOVING Y_YOUTH AS ITS P-VALUE IS > 0.05
#glmTest_FINAL<- glm(Choice ~  Gender + Amount_purchased + 
 #   Frequency +  + P_Child  + 
 #   P_Cook + P_DIY + P_Art
 #           , data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
#s#ummary(glmTest_FINAL)
#vif(glmTest_FINAL)

# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glm1, newdata = test, type = 'response')

#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)

caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and , positive = '1'

## 89.52 Accuracy
## 39.89 Sensitivity
## 93.81 Specificity

```

## Finding most optimal cutoff for highest sensitiviy & Specificity
```{r}
test$Pred_Y <- ifelse(test$PredProb >= 0.8, 1, 0)

caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and , positive = '1'

## 91.35 Accuracy
## 64.71 Sensitivity
## 91.55 Specificity


```



## THIS DID NOT IMPROVE MODEL SO WILL NOT CONTINUE WITH THIS
### improving the model and creating some variables
```{r}
head(train)
summary(train$First_purchase)
summary(train$Last_purchase)

train$Fst_purch_bckt <- 
  factor(
    case_when(
    train$First_purchase <= 12 ~ '0-12 Mths'
    ,train$First_purchase > 12 & train$First_purchase <= 18 ~ '13-18 Mths'
    ,train$First_purchase > 18 & train$First_purchase <= 30 ~ '19-30 Mths'
    ,train$First_purchase > 30 ~ '>= 31 Mths'
    ))

train$lst_purch_bckt <- 
  factor(
    case_when(
    train$Last_purchase <= 1 ~ '1 Mths'
    ,train$Last_purchase > 1 & train$Last_purchase <= 2 ~ '2 Mths'
    ,train$Last_purchase > 2 & train$Last_purchase <= 4 ~ '3-4 Mths'
    ,train$Last_purchase > 4 ~ '>= 5 Mths'
    ))

train$P_Child_bck <- 
  factor(
    case_when(
    train$P_Child == 0 ~ '0'
    ,train$P_Child == 1 ~ '1'
    ,train$P_Child == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

train$P_Youth_bck <- 
  factor(
    case_when(
    train$P_Youth == 0 ~ '0'
    ,train$P_Youth == 1 ~ '1'
    ,train$P_Youth == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))


train$P_Cook_bck <- 
  factor(
    case_when(
    train$P_Cook == 0 ~ '0'
    ,train$P_Cook == 1 ~ '1'
    ,train$P_Cook == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))


train$P_DIY_bck <- 
  factor(
    case_when(
    train$P_DIY == 0 ~ '0'
    ,train$P_DIY == 1 ~ '1'
    ,train$P_DIY == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

train$P_Art_bck <- 
  factor(
    case_when(
    train$P_Art == 0 ~ '0'
    ,train$P_Art == 1 ~ '1'
    ,train$P_Art == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

str(train)



#SAME DATA MANIPULATION ON test DATA IN ORDER TO PREDICT ON IT
test$Fst_purch_bckt <- 
  factor(
    case_when(
    test$First_purchase <= 12 ~ '0-12 Mths'
    ,test$First_purchase > 12 & test$First_purchase <= 18 ~ '13-18 Mths'
    ,test$First_purchase > 18 & test$First_purchase <= 30 ~ '19-30 Mths'
    ,test$First_purchase > 30 ~ '>= 31 Mths'
    ))

test$lst_purch_bckt <- 
  factor(
    case_when(
    test$Last_purchase <= 1 ~ '1 Mths'
    ,test$Last_purchase > 1 & test$Last_purchase <= 2 ~ '2 Mths'
    ,test$Last_purchase > 2 & test$Last_purchase <= 4 ~ '3-4 Mths'
    ,test$Last_purchase > 4 ~ '>= 5 Mths'
    ))

test$P_Child_bck <- 
  factor(
    case_when(
    test$P_Child == 0 ~ '0'
    ,test$P_Child == 1 ~ '1'
    ,test$P_Child == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

test$P_Youth_bck <- 
  factor(
    case_when(
    test$P_Youth == 0 ~ '0'
    ,test$P_Youth == 1 ~ '1'
    ,test$P_Youth == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))


test$P_Cook_bck <- 
  factor(
    case_when(
    test$P_Cook == 0 ~ '0'
    ,test$P_Cook == 1 ~ '1'
    ,test$P_Cook == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))


test$P_DIY_bck <- 
  factor(
    case_when(
    test$P_DIY == 0 ~ '0'
    ,test$P_DIY == 1 ~ '1'
    ,test$P_DIY == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

test$P_Art_bck <- 
  factor(
    case_when(
    test$P_Art == 0 ~ '0'
    ,test$P_Art == 1 ~ '1'
    ,test$P_Art == 2 ~ '2'
    ,TRUE ~ '>= 3'
  ))

str(test)
```


### logistic model
```{r}
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm2 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook 
            -P_DIY-P_Art
            , data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm2)
vif(glm2)


#FINAL MODEL FOR TEST DATA - TO GET PREDICTIONS
glmf <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook 
            -P_DIY-P_Art
            , data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmf)
vif(glmf)


# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glmf, newdata = test, type = 'response')

#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)

caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and 

## 91.57 Accuracy
## 62.5 Sensitivity
## 92.08 Specificity

```




### EVERYTHING BELOW THIS IS FOR THE FINAL REPORT


# Results:

Here we will be discussing the outputs from our logistic model we applied to the data set to best predict if someone will purchase a book - Choice (1/0). We first begin with all the variables in the data set to see which independent variables are significant.

```{r}
train <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-train.xlsx")
test <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-test.xlsx")
glm1 <- glm(Choice ~ . - Observation, data = train)
summary(glm1)
vif(glm1)
```

With all the dependent variables, last_Purchase has the largest VIF so we decide to remove that from our model.
```{r}
glm1 <- glm(Choice ~ .-Observation-Last_purchase, data = train)
summary(glm1)
vif(glm1)
```
First_purchase alos has a large VIF so we will remove that from our model.
```{r}
glm1 <- glm(Choice ~ .-Observation-Last_purchase-First_purchase, data = train)
summary(glm1)
vif(glm1)
```
Finally, P_Youth has a p-value > 0.05 so we will remove that to have our final model

```{r}
glm1 <- glm(Choice ~ .-Observation-Last_purchase-First_purchase-P_Youth, data = train)
summary(glm1)
vif(glm1)
```

Before we get into the confusion matrix. Lets explain the relationship of each independent variable to the dependent variable. We first compute the exponential of our coefficient ratio to get the odds ratio.

```{r}
exp(coef(glm1)['Gender'])
exp(coef(glm1)['Amount_purchased'])
exp(coef(glm1)['Frequency'])
exp(coef(glm1)['P_Child'])
exp(coef(glm1)['P_Cook'])
exp(coef(glm1)['P_DIY'])
exp(coef(glm1)['P_Art'])

```

we see Males decrease the odds of a client buying a book by a factor of 0.88

we see for those with a larger amount of books purchased increases the odds of a client buying a book by a factor of 1.

we see for those with a higher frequency of books purchased decreases the odds of a client buying a book by a factor of 0.99

we see for those with a higher purchase of child books increases the odds of a client buying a book by a factor of 0.97

we see for those with a higher purchase of DIY books increases the odds of a client buying a book by a factor of 0.96

we see for those with a higher purchase of Art books increases the odds of a client buying a book by a factor of 1.24



With this model, we then use it on our test sample to see how well it predicts and determine the most optimal cutoff to have the highest Sensitivity. Here are the following results:
```{r, warning=FALSE}

# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glm1, newdata = test, type = 'response')

#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)

caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and , positive = '1'

## 89.96 Accuracy
## 41.29 Sensitivity
## 93.47 Specificity

#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.8, 1, 0)

caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and , positive = '1'

## 91.3 Accuracy
## 83 Sensitivity
## 91.3 Specificity

```


We see the best cutoff for the highest sensitivity  is at 0.8. With this, the performance of our model on the test data set is 91% accurate overall, and our sensitivity is 83% with a specificity of 91%


---
title: "Case Study 1"
author: "Alex Martinez"
date: "2024-09-10"
output: pdf_document
---

```{r libraries, include=FALSE}
#| echo: false
library(caret)
library(here)
library(lattice)
library(ggplot2)
library(logistf)
library(MASS)
library(tidyverse)
library(corrplot)
library(car)
library(ROCR)
library(dplyr)
```

```{r Loading Data, warning=FALSE}
#| echo: false
getwd() #open R project file to have the same file path
df1 = read.csv2('Data/bank-additional.csv') # Reading the csv from the location above

```


## Data Structure

```{r Data Structure}
#| echo: false
str(df1) #probably remove from final paper
```

## Data Cleanup

```{r Data Cleanup}
#| echo: false
#change y into factor 
df1$y<-as.factor(df1$y)
df1$job<- as.factor(df1$job)
df1$marital <- as.factor(df1$marital)
df1$education <- as.factor(df1$education)
df1$default <- as.factor(df1$default)
df1$housing<- as.factor(df1$housing)
df1$loan <- as.factor(df1$loan)
df1$contact <- as.factor(df1$contact)
df1$month <- as.factor(df1$month)
df1$day_of_week <- as.factor(df1$day_of_week)
df1$poutcome <- as.factor(df1$poutcome)
df1$emp.var.rate <- as.numeric(df1$emp.var.rate)
df1$cons.price.idx <- as.numeric(df1$cons.price.idx)
df1$cons.conf.idx <- as.numeric(df1$cons.conf.idx)
df1$euribor3m <- as.numeric(df1$euribor3m)
df1$nr.employed <- as.integer(df1$nr.employed)
unique(df1$previous)
unique(df1$pdays)
df1$y_int <- ifelse(df1$y == 'yes', 1, 0)
df1$ppdays_ind <- ifelse(df1$pdays == 999, 0, 1)
df1$married_ind <- ifelse(df1$marital == 'married', 1, 0)


#Pdays - make it an indicator of not/yes
#duration - amt of time you have them, so same as Y 

```

## Data Exploration

### Correlation

```{r Correlation}
#| echo: false
# is there high correlation in any of our numeric variables
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
#looks like there is higher correlation between pdays and previous but it is under .6 
corrplot(M, method = c("color"))

```

### Unbalanced Dataset

```{r Unbalanced Dataset}
#| echo: false
#data is unbalanced 3.6k no, 451 yes
summary(df1$y)
```

### Distribution (Numeric Variables)

```{r Distribution of Numerical Variables}
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long = df1 %>% 
  dplyr::select(age, campaign, previous, emp.var.rate, cons.price.idx, 
                cons.conf.idx, euribor3m, nr.employed) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

#facet view
ggplot(bank_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "gray", color = "black") +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Variables", x = NULL, y = "Frequency") +
  theme(strip.text = element_text(face = "bold"))
```

### Distribution (Categorical Variables)

```{r Distribution of Categorical Variables}
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long_cat = df1 %>% 
  dplyr::select(job, marital, education, default, housing, loan, contact, poutcome) %>% 
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

#facet view
ggplot(bank_long_cat, aes(x = Value)) +
  geom_bar(fill = "gray", color = "black") +
  facet_wrap(~ Variable, scales = "free", ncol = 3) +
  theme_minimal() +
  labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
  theme(strip.text = element_text(face = "bold"), axis.text.x = element_text(angle = 45, hjust = 1))

```

# Results:

*Presents and discusses the results from model(s). Discusses relationships between covariates and response, if possible, and provides deep insights behind relationships in the context of the application.*

# Conclusions:

*Concludes with a summary of the aim and results. Discusses alternative methods that can be used.*

```{r Variable Selection}
#keep variables I want for Modeling
df2 <- df1 %>% select(age,job, married_ind, education, default, housing, loan, contact, campaign, 
                      ppdays_ind, previous, poutcome, emp.var.rate, cons.price.idx, cons.conf.idx, 
                      euribor3m, nr.employed, y, y_int) 
```

```{r Balancing Dataset}
# since data is unbalanced we should take a more balanced sample
set.seed(42) 
df_y_y = df2 %>% filter(y == "yes")
df_y_n = df2 %>% filter(y == "no")
sample_y = sample_n(df_y_n, nrow(df_y_y))
df_bal = rbind(df_y_y,sample_y)
```

```{r Data Splitting}
# test and train split
df_bal_split <- sample(nrow(df_bal),0.8*nrow(df_bal),replace = F) # Setting training sample to be 80% of the data
df_train <- df_bal[df_bal_split,]
df_test <- df_bal[-df_bal_split,]
```

```{r model building}
# creating binary numeric variable for linear model
str(df2)

# first exploring with backwards step selection on lm with full data
lm1 <- lm(y_int ~ . -y, data = df_train)
summary(lm1)

lm1_step <- step(lm1, direction = "backward")
summary(lm1_step)

# final lm model base off of step 
lmf <- lm(y_int ~ age + contact + campaign + poutcome + emp.var.rate + 
    cons.price.idx + cons.conf.idx, data = df_train)
#summary(lmf)

vif(lmf) # confirm there is no co linearity 
# R- Squared is very small at 27% but is expectedc since y is binomial


### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(y_int ~ . -y, data = df_train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)

# using backwards Step Selection
glm_step = step(glm1, direction = "backward") # stepwise backward elim.
summary(glm_step)

glmf <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome + 
    emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial, 
    data = df_train)
summary(glmf)

vif(glmf) # no colinearity detected

# Predict the probability on TEST - cutoff 0.5
df_test$PredProb <- predict.glm(glmf, newdata = df_test, type = 'response')

#create Prediction Indicators for y
df_test$Pred_Y <- ifelse(df_test$PredProb >= 0.5, 1, 0)

caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 74%
# sens: 80%
# Spec: 70%

plot(glmf)


##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY

# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome + 
    emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial, 
    data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

#### end of default cutoff of 0.5
#### optimal cutoff

#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int) 
pred
#Predicted Probability and True Classification

# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc

#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))

# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values), 
     type="l", lwd=2, 
     ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))

par(new=TRUE) # plot another line in same plot

#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values), 
     type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')

#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity

abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)





##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%


## best sensitivity
df_test$Pred_Y_sens <- ifelse(df_test$PredProb >= 0.3, 1, 0)
caret::confusionMatrix(as.factor(df_test$Pred_Y_sens), as.factor(df_test$y_int), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 60%
# sens: 85%
# Spec: 34%



```
# REPORTING OUTLINE/UPDATES


# Bank Marketing Case Study

# Executive Summary:

*Brief introduction of problem. Summarizes key findings. Summarizes insights behind key findings.*

# Our Problem:

*Clear description of the problem, from an application and theoretical point of view. Outlines the report.*

# Literature Review:

*Discusses and cites existing works in the theoretical and application realm.*

# Methods:

*Discusses types of variables, sample size, and sampling techniques (if any). Discusses the model(s) and its assumptions and limitations.*

In this case study, the dataset contains 21 variables related to client demographic and financial information, with a total of 4,119 observations. To achieve our objectives, we applied two methodologies: Linear Regression (LM) and Generalized Linear Model (GLM).



# Data:

# Results:
### Presents and discusses the results from model(s). Discusses relationships between covariates and response, if possible, and provides deep insights behind relationships in the context of the application.

Here we will be discussing the outputs from our two models we applied to the data set to best predict if a client will subscribe (yes/no) to a term deposit (y). We first begin with a simple linear model and use the backwards selection method to keep only the significant variables. Here is the output:
```{r}
summary(lmf)
```
We see that the model kept age, contact, campaign, poutcome, emp.var.rate, cons.price.idx, and cons.conf.idx.
Right away, we see that the model is significant but the Adjusted R-Square is pretty low at 27%. This is expected since our dependent variable is binary and a Linear regression model is not the best option for binary outcomes.

With this conclusion we move on to a Logistic regression model since this gives us a prediction for our dependent variable. This will help us predict if a client will subscribe to a term deposit (indicated as 1).

The final model that is created using the backwards selection method is the follwoing:
```{r}
summary(glmf)
```
Before we get into the confusion matrix. Lets explain the relationship of each independent variable to the dependent variable. We first compute the exponential of our coefficient ratio to get the odds ratio.
```{r}
exp(coef(glmf)['age'])
exp(coef(glmf)['contacttelephone'])
exp(coef(glmf)['campaign            '])
exp(coef(glmf)['previous             '])
exp(coef(glmf)['poutcomenonexistent  '])
exp(coef(glmf)['poutcomesuccess      '])
exp(coef(glmf)['emp.var.rate'])
exp(coef(glmf)['cons.price.idx'])
exp(coef(glmf)['cons.conf.idx'])

```

we see an extra year of age increased the odds of a client subscribing to a term deposit  by a factor of 1.02

we see for those with contacttelephone increases the odds of a client subscribing to a term deposit  by a factor of 0.38.

we see an extra increase of emp.var.rate increased the odds of a client subscribing to a term deposit  by a factor of 0.41

we see an extra increaase of cons.price.idx increased the odds of a client subscribing to a term deposit  by a factor of 6.12.

we see an extra increase of cons.conf.idx increased the odds of a client subscribing to a term deposit  by a factor of 1.06.




With this model, we then use it on our test sample to see how well it predicts and determine the most optimal cutoff to have the highest Specificity and Sensitivity. Here are the following results:
```{r}
# Predict the probability (p) of 
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome + 
    emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial, 
    data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

#### end of default cutoff of 0.5
#### optimal cutoff

#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int) 
pred
#Predicted Probability and True Classification

# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc

#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))

# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values), 
     type="l", lwd=2, 
     ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))

par(new=TRUE) # plot another line in same plot

#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values), 
     type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')

#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity

abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)


##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%
``` 
We see the best cutoff for the highest sensitivity and specificity is at 0.46. With this, the performance of our model on the test data set is 73% accurate overall, and our sensitivity is 77% with a specificity of 70%







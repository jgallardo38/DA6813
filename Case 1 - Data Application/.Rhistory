))
train$P_Child_bck <-
factor(
case_when(
train$P_Child == 0 ~ '0'
,train$P_Child == 1 ~ '1'
,train$P_Child == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Youth_bck <-
factor(
case_when(
train$P_Youth == 0 ~ '0'
,train$P_Youth == 1 ~ '1'
,train$P_Youth == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Cook_bck <-
factor(
case_when(
train$P_Cook == 0 ~ '0'
,train$P_Cook == 1 ~ '1'
,train$P_Cook == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_DIY_bck <-
factor(
case_when(
train$P_DIY == 0 ~ '0'
,train$P_DIY == 1 ~ '1'
,train$P_DIY == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Art_bck <-
factor(
case_when(
train$P_Art == 0 ~ '0'
,train$P_Art == 1 ~ '1'
,train$P_Art == 2 ~ '2'
,TRUE ~ '>= 3'
))
str(train)
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
vif(glm1)
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glm1, newdata = test, type = 'response')
glm2 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm2)
vif(glm2)
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glm2, newdata = train, type = 'response')
test
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glm2, newdata = test, type = 'response')
summary(glm2)
glm_step = step(glm2, direction = "backward") # stepwise backward elim.
summary(glm_step)
# Predict the probability on train - cutoff 0.5
train$PredProb <- predict.glm(glm1, newdata = test, type = 'response')
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glm1, newdata = test, type = 'response')
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glm1, newdata = train, type = 'response')
View(test)
#create Prediction Indicators for y
train$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
head(train)
summary(train$First_purchase)
summary(train$Last_purchase)
train$Fst_purch_bckt <-
factor(
case_when(
train$First_purchase <= 12 ~ '0-12 Mths'
,train$First_purchase > 12 & train$First_purchase <= 18 ~ '13-18 Mths'
,train$First_purchase > 18 & train$First_purchase <= 30 ~ '19-30 Mths'
,train$First_purchase > 30 ~ '>= 31 Mths'
))
train$lst_purch_bckt <-
factor(
case_when(
train$Last_purchase <= 1 ~ '1 Mths'
,train$Last_purchase > 1 & train$Last_purchase <= 2 ~ '2 Mths'
,train$Last_purchase > 2 & train$Last_purchase <= 4 ~ '3-4 Mths'
,train$Last_purchase > 4 ~ '>= 5 Mths'
))
train$P_Child_bck <-
factor(
case_when(
train$P_Child == 0 ~ '0'
,train$P_Child == 1 ~ '1'
,train$P_Child == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Youth_bck <-
factor(
case_when(
train$P_Youth == 0 ~ '0'
,train$P_Youth == 1 ~ '1'
,train$P_Youth == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Cook_bck <-
factor(
case_when(
train$P_Cook == 0 ~ '0'
,train$P_Cook == 1 ~ '1'
,train$P_Cook == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_DIY_bck <-
factor(
case_when(
train$P_DIY == 0 ~ '0'
,train$P_DIY == 1 ~ '1'
,train$P_DIY == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Art_bck <-
factor(
case_when(
train$P_Art == 0 ~ '0'
,train$P_Art == 1 ~ '1'
,train$P_Art == 2 ~ '2'
,TRUE ~ '>= 3'
))
str(train)
#SAME DATA MANIPULATION ON test DATA IN ORDER TO PREDICT ON IT
test$Fst_purch_bckt <-
factor(
case_when(
test$First_purchase <= 12 ~ '0-12 Mths'
,test$First_purchase > 12 & test$First_purchase <= 18 ~ '13-18 Mths'
,test$First_purchase > 18 & test$First_purchase <= 30 ~ '19-30 Mths'
,test$First_purchase > 30 ~ '>= 31 Mths'
))
test$lst_purch_bckt <-
factor(
case_when(
test$Last_purchase <= 1 ~ '1 Mths'
,test$Last_purchase > 1 & test$Last_purchase <= 2 ~ '2 Mths'
,test$Last_purchase > 2 & test$Last_purchase <= 4 ~ '3-4 Mths'
,test$Last_purchase > 4 ~ '>= 5 Mths'
))
test$P_Child_bck <-
factor(
case_when(
test$P_Child == 0 ~ '0'
,test$P_Child == 1 ~ '1'
,test$P_Child == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Youth_bck <-
factor(
case_when(
test$P_Youth == 0 ~ '0'
,test$P_Youth == 1 ~ '1'
,test$P_Youth == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Cook_bck <-
factor(
case_when(
test$P_Cook == 0 ~ '0'
,test$P_Cook == 1 ~ '1'
,test$P_Cook == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_DIY_bck <-
factor(
case_when(
test$P_DIY == 0 ~ '0'
,test$P_DIY == 1 ~ '1'
,test$P_DIY == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Art_bck <-
factor(
case_when(
test$P_Art == 0 ~ '0'
,test$P_Art == 1 ~ '1'
,test$P_Art == 2 ~ '2'
,TRUE ~ '>= 3'
))
str(test)
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm2 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm2)
vif(glm2)
glm2 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm2)
vif(glm2)
glmf <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmf)
vif(glmf)
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glmf, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child + P_Youth +
P_Cook + P_DIY + P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
vif(glm1)
# Predict the probability on train - cutoff 0.5
train$PredProb <- predict.glm(glm1, newdata = train, type = 'response')
#create Prediction Indicators for y
train$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
glmT <- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child + P_Youth +
P_Cook + P_DIY + P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmT)
vif(glmT)
#CREATING MODEL ON TEST TO USE AND GET PREDICTIONS
glmT <- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child + P_Youth +
P_Cook + P_DIY + P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmT)
vif(glmT)
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glmT, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
glmf <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmf)
vif(glmf)
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glmf, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
library(tidyverse)
library(cluster)
library(factoextra)
library(dendextend)
library(readxl)
library(dplyr)
library(car)
library(ggplot2)
getwd() #open R project file to have the same file path
train <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-train.xlsx")
test <- read_xlsx("C:/Users/alexm/Downloads/Pract 2/DA6813/Data/BBBC-test.xlsx")
head(train)
str(train)
histogram(train$Amount_purchased)
histogram(train$Frequency)
histogram(train$Last_purchase)
histogram(train$First_purchase)
histogram(train$P_Youth)
histogram(train$P_DIY)
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child + P_Youth +
P_Cook + P_DIY + P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
vif(glm1)
#CREATING MODEL ON TEST TO USE AND GET PREDICTIONS
glmT <- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child + P_Youth +
P_Cook + P_DIY + P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmT)
vif(glmT)
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glmT, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
## 91.48 Accuracy
## 61.11 Sensitivity
## 91.96 Specificity
head(train)
summary(train$First_purchase)
summary(train$Last_purchase)
train$Fst_purch_bckt <-
factor(
case_when(
train$First_purchase <= 12 ~ '0-12 Mths'
,train$First_purchase > 12 & train$First_purchase <= 18 ~ '13-18 Mths'
,train$First_purchase > 18 & train$First_purchase <= 30 ~ '19-30 Mths'
,train$First_purchase > 30 ~ '>= 31 Mths'
))
train$lst_purch_bckt <-
factor(
case_when(
train$Last_purchase <= 1 ~ '1 Mths'
,train$Last_purchase > 1 & train$Last_purchase <= 2 ~ '2 Mths'
,train$Last_purchase > 2 & train$Last_purchase <= 4 ~ '3-4 Mths'
,train$Last_purchase > 4 ~ '>= 5 Mths'
))
train$P_Child_bck <-
factor(
case_when(
train$P_Child == 0 ~ '0'
,train$P_Child == 1 ~ '1'
,train$P_Child == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Youth_bck <-
factor(
case_when(
train$P_Youth == 0 ~ '0'
,train$P_Youth == 1 ~ '1'
,train$P_Youth == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Cook_bck <-
factor(
case_when(
train$P_Cook == 0 ~ '0'
,train$P_Cook == 1 ~ '1'
,train$P_Cook == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_DIY_bck <-
factor(
case_when(
train$P_DIY == 0 ~ '0'
,train$P_DIY == 1 ~ '1'
,train$P_DIY == 2 ~ '2'
,TRUE ~ '>= 3'
))
train$P_Art_bck <-
factor(
case_when(
train$P_Art == 0 ~ '0'
,train$P_Art == 1 ~ '1'
,train$P_Art == 2 ~ '2'
,TRUE ~ '>= 3'
))
str(train)
#SAME DATA MANIPULATION ON test DATA IN ORDER TO PREDICT ON IT
test$Fst_purch_bckt <-
factor(
case_when(
test$First_purchase <= 12 ~ '0-12 Mths'
,test$First_purchase > 12 & test$First_purchase <= 18 ~ '13-18 Mths'
,test$First_purchase > 18 & test$First_purchase <= 30 ~ '19-30 Mths'
,test$First_purchase > 30 ~ '>= 31 Mths'
))
test$lst_purch_bckt <-
factor(
case_when(
test$Last_purchase <= 1 ~ '1 Mths'
,test$Last_purchase > 1 & test$Last_purchase <= 2 ~ '2 Mths'
,test$Last_purchase > 2 & test$Last_purchase <= 4 ~ '3-4 Mths'
,test$Last_purchase > 4 ~ '>= 5 Mths'
))
test$P_Child_bck <-
factor(
case_when(
test$P_Child == 0 ~ '0'
,test$P_Child == 1 ~ '1'
,test$P_Child == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Youth_bck <-
factor(
case_when(
test$P_Youth == 0 ~ '0'
,test$P_Youth == 1 ~ '1'
,test$P_Youth == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Cook_bck <-
factor(
case_when(
test$P_Cook == 0 ~ '0'
,test$P_Cook == 1 ~ '1'
,test$P_Cook == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_DIY_bck <-
factor(
case_when(
test$P_DIY == 0 ~ '0'
,test$P_DIY == 1 ~ '1'
,test$P_DIY == 2 ~ '2'
,TRUE ~ '>= 3'
))
test$P_Art_bck <-
factor(
case_when(
test$P_Art == 0 ~ '0'
,test$P_Art == 1 ~ '1'
,test$P_Art == 2 ~ '2'
,TRUE ~ '>= 3'
))
str(test)
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm2 <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm2)
vif(glm2)
#FINAL MODEL FOR TEST DATA - TO GET PREDICTIONS
glmf <- glm(Choice ~ . -Observation -Last_purchase -First_purchase - P_Youth - P_Child - P_Cook
-P_DIY-P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmf)
vif(glmf)
# Predict the probability on TEST - cutoff 0.5
test$PredProb <- predict.glm(glmf, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
## 91.57 Accuracy
## 62.5 Sensitivity
## 92.08 Specificity
##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY
# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
glmf
summary(glmf)
#REMOVING Y_YOUTH AS ITS P-VALUE IS > 0.05
glmTest_FINAL<- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child  +
P_Cook + P_DIY + P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmT)
glmTest_FINAL<- glm(Choice ~  Gender + Amount_purchased +
Frequency +  + P_Child  +
P_Cook + P_DIY + P_Art
, data = test, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glmTest_FINAL)
vif(glmTest_FINAL)
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glmTest_FINAL, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
summary(glmTest_FINAL)
probabilities <- predict(glmTest_FINAL, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#ROC Curve and AUC
pred <- prediction(probabilities,test$Choice)
pred
# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values),
type="l", lwd=2,
ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))
par(new=TRUE) # plot another line in same plot
#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values),
type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')
#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity
abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)
# Predict the probability on train - cutoff 0.5
test$PredProb <- predict.glm(glmTest_FINAL, newdata = test, type = 'response')
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.08, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.8, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
test$Pred_Y <- ifelse(test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
#create Prediction Indicators for y
test$Pred_Y <- ifelse(test$PredProb >= 0.08, 1, 0)
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '1') #this function and
caret::confusionMatrix(as.factor(test$Choice),as.factor(test$Pred_Y), positive = '0') #this function and

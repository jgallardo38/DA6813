---
title: "Case Study 1"
author: "Alex Martinez"
date: "2024-09-10"
output: pdf_document
---

```{r libraries, include=FALSE}
#| echo: false
library(caret)
library(here)
library(lattice)
library(ggplot2)
library(logistf)
library(MASS)
library(tidyverse)
library(corrplot)
library(car)
library(ROCR)
library(dplyr)
```

```{r Loading Data, warning=FALSE}
#| echo: false
getwd() #open R project file to have the same file path
df1 = read.csv2('Data/bank-additional.csv') # Reading the csv from the location above

```


## Data Structure

```{r Data Structure}
#| echo: false
str(df1) #probably remove from final paper
```

## Data Cleanup

```{r Data Cleanup}
#| echo: false
#change y into factor 
df1$y<-as.factor(df1$y)
df1$job<- as.factor(df1$job)
df1$marital <- as.factor(df1$marital)
df1$education <- as.factor(df1$education)
df1$default <- as.factor(df1$default)
df1$housing<- as.factor(df1$housing)
df1$loan <- as.factor(df1$loan)
df1$contact <- as.factor(df1$contact)
df1$month <- as.factor(df1$month)
df1$day_of_week <- as.factor(df1$day_of_week)
df1$poutcome <- as.factor(df1$poutcome)
df1$emp.var.rate <- as.numeric(df1$emp.var.rate)
df1$cons.price.idx <- as.numeric(df1$cons.price.idx)
df1$cons.conf.idx <- as.numeric(df1$cons.conf.idx)
df1$euribor3m <- as.numeric(df1$euribor3m)
df1$nr.employed <- as.integer(df1$nr.employed)
unique(df1$previous)
unique(df1$pdays)
df1$y_int <- ifelse(df1$y == 'yes', 1, 0)
df1$ppdays_ind <- ifelse(df1$pdays == 999, 0, 1)
df1$married_ind <- ifelse(df1$marital == 'married', 1, 0)


#Pdays - make it an indicator of not/yes
#duration - amt of time you have them, so same as Y 

```

## Data Exploration

### Correlation

```{r Correlation}
#| echo: false
# is there high correlation in any of our numeric variables
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
#looks like there is higher correlation between pdays and previous but it is under .6 
corrplot(M, method = c("color"))

```

### Unbalanced Dataset

```{r Unbalanced Dataset}
#| echo: false
#data is unbalanced 3.6k no, 451 yes
summary(df1$y)
```

### Distribution (Numeric Variables)

```{r Distribution of Numerical Variables}
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long = df1 %>% 
  dplyr::select(age, campaign, previous, emp.var.rate, cons.price.idx, 
                cons.conf.idx, euribor3m, nr.employed) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

#facet view
ggplot(bank_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "gray", color = "black") +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Variables", x = NULL, y = "Frequency") +
  theme(strip.text = element_text(face = "bold"))
```

### Distribution (Categorical Variables)

```{r Distribution of Categorical Variables}
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long_cat = df1 %>% 
  dplyr::select(job, marital, education, default, housing, loan, contact, poutcome) %>% 
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

#facet view
ggplot(bank_long_cat, aes(x = Value)) +
  geom_bar(fill = "gray", color = "black") +
  facet_wrap(~ Variable, scales = "free", ncol = 3) +
  theme_minimal() +
  labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
  theme(strip.text = element_text(face = "bold"), axis.text.x = element_text(angle = 45, hjust = 1))

```

# Results:

*Presents and discusses the results from model(s). Discusses relationships between covariates and response, if possible, and provides deep insights behind relationships in the context of the application.*

# Conclusions:

*Concludes with a summary of the aim and results. Discusses alternative methods that can be used.*

```{r Variable Selection}
#keep variables I want for Modeling
df2 <- df1 %>% select(age,job, married_ind, education, default, housing, loan, contact, campaign, 
                      ppdays_ind, previous, poutcome, emp.var.rate, cons.price.idx, cons.conf.idx, 
                      euribor3m, nr.employed, y, y_int) 
```

```{r Balancing Dataset}
# since data is unbalanced we should take a more balanced sample
set.seed(42) 
df_y_y = df2 %>% filter(y == "yes")
df_y_n = df2 %>% filter(y == "no")
sample_y = sample_n(df_y_n, nrow(df_y_y))
df_bal = rbind(df_y_y,sample_y)
```

```{r Data Splitting}
# test and train split
df_bal_split <- sample(nrow(df_bal),0.8*nrow(df_bal),replace = F) # Setting training sample to be 80% of the data
df_train <- df_bal[df_bal_split,]
df_test <- df_bal[-df_bal_split,]
```

```{r model building}
# creating binary numeric variable for linear model
str(df2)

# first exploring with backwards step selection on lm with full data
lm1 <- lm(y_int ~ . -y, data = df_train)
summary(lm1)

lm1_step <- step(lm1, direction = "backward")
summary(lm1_step)

# final lm model base off of step 
lmf <- lm(y_int ~ age + contact + campaign + poutcome + emp.var.rate + 
    cons.price.idx + cons.conf.idx, data = df_train)
#summary(lmf)

vif(lmf) # confirm there is no co linearity 
# R- Squared is very small at 27% but is expectedc since y is binomial


### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(y_int ~ . -y, data = df_train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)

# using backwards Step Selection
glm_step = step(glm1, direction = "backward") # stepwise backward elim.
summary(glm_step)

glmf <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome + 
    emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial, 
    data = df_train)
summary(glmf)

vif(glmf) # no colinearity detected

# Predict the probability on TEST - cutoff 0.5
df_test$PredProb <- predict.glm(glmf, newdata = df_test, type = 'response')

#create Prediction Indicators for y
df_test$Pred_Y <- ifelse(df_test$PredProb >= 0.5, 1, 0)

caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 74%
# sens: 80%
# Spec: 70%

plot(glmf)


##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY

# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome + 
    emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial, 
    data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)

#### end of default cutoff of 0.5
#### optimal cutoff

#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int) 
pred
#Predicted Probability and True Classification

# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc

#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))

# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values), 
     type="l", lwd=2, 
     ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))

par(new=TRUE) # plot another line in same plot

#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values), 
     type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')

#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity

abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)





##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%


## best sensitivity
df_test$Pred_Y_sens <- ifelse(df_test$PredProb >= 0.3, 1, 0)
caret::confusionMatrix(as.factor(df_test$Pred_Y_sens), as.factor(df_test$y_int), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 60%
# sens: 85%
# Spec: 34%



```
# REPORTING OUTLINE/UPDATES


# Bank Marketing Case Study

# Executive Summary:

Key Findings
Client Behavior Insights: Features related to client behavior, such as the number of previous contacts and the duration of the last call, strongly influence the likelihood of subscription. A longer call duration often correlates with a positive outcome.

Macroeconomic Factors: Variables such as employment variation rate and the Euribor rate (a 3-month interest rate) also impact the decision. A positive employment outlook increases subscription rates.

Campaign Features: The total number of contacts and the success of previous campaigns (poutcome) are critical indicators for future success. Clients who had successful outcomes in previous campaigns were more likely to subscribe again.

*Brief introduction of problem. Summarizes key findings. Summarizes insights behind key findings.*

# Our Problem:

Problem Description (Application and Theoretical)
From an application perspective, this problem addresses optimizing marketing campaign effectiveness for financial institutions. By accurately predicting the likelihood of a subscription, banks can reduce unnecessary contact costs, improve customer targeting, and maximize return on investment in marketing efforts.

From a theoretical standpoint, this problem falls into the domain of binary classification and predictive analytics. Models such as logistic regression, decision trees, and machine learning classifiers like random forests can be applied. The use of client behavior and macroeconomic data offers a rich feature space that improves model accuracy. Theoretical considerations such as class imbalance, feature importance, and multicollinearity play important roles in model selection and evaluation.

*Clear description of the problem, from an application and theoretical point of view. Outlines the report.*

# Literature Review:

Existing Works in Theoretical and Application Realm
Research in customer retention and subscription prediction in financial services has been extensive. Theoretical studies such as Predictive Modeling for Marketing Campaigns by Chapman et al. (2015) highlight the utility of machine learning in optimizing campaign performance. Studies like Moro et al. (2014) applied data mining techniques on a similar Portuguese banking dataset, showing that models like logistic regression and decision trees provide high accuracy in predicting term deposit subscriptions.

Further, works such as Customer Analytics in Financial Services by Homburg et al. (2016) validate the impact of socioeconomic variables like employment rates and consumer confidence on consumer financial decisions. These existing works provide a solid foundation for applying predictive models in this case study.

By leveraging these insights, this project aims to contribute to the growing body of research that helps financial institutions fine-tune their marketing strategies through data-driven approaches.

*Discusses and cites existing works in the theoretical and application realm.*

# Methods:

*Discusses types of variables, sample size, and sampling techniques (if any). Discusses the model(s) and its assumptions and limitations.*

In this case study, the dataset contains 21 variables related to client demographic and financial information, with a total of 4,119 observations. To achieve our objectives, we applied two methodologies: Linear Regression (LM) and Generalized Linear Model (GLM).



# Data:

# Results:
### Presents and discusses the results from model(s). Discusses relationships between covariates and response, if possible, and provides deep insights behind relationships in the context of the application.

<<<<<<< HEAD
Here we will be discussing the outputs from our two models we applied to the data set to best predict if a client will subscribe (yes/no) to a term deposit (y). We first begin with a simple linear model using all the variables we decided to select for our modeling. Here is the following output from our linear model.
=======
<<<<<<< HEAD
Here we will be discussing the outputs from our two models we applied to the data set to best predict if a client will subscribe (yes/no) to a term deposit (y). We first begin with a simple linear model using all the variables we decided to select for our modeling. Here is the following output for our linear model.
=======
Here we will be discussing the outputs from our two models we applied to the data set to best predict if a client will subscribe (yes/no) to a term deposit (y). We first begin with a simple linear model using all the variables we decided to select for our modeling. Here is the following output from our linear model.
>>>>>>> origin/main
>>>>>>> origin/main



*Discusses how data was handled, i.e. cleaned and preprocessed. Discusses distributions, correlations, etc.*



df_test <- df_bal[-df_bal_split,]
# creating binary numeric variable for linear model
str(df2)
# first exploring with backwards step selection on lm with full data
lm1 <- lm(y_int ~ . -y, data = df_train)
summary(lm1)
lm1_step <- step(lm1, direction = "backward")
summary(lm1_step)
# final lm model base off of step
lmf <- lm(y_int ~ age + contact + campaign + poutcome + emp.var.rate +
cons.price.idx + cons.conf.idx, data = df_train)
#summary(lmf)
vif(lmf) # confirm there is no co linearity
# R- Squared is very small at 27% but is expectedc since y is binomial
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(y_int ~ . -y, data = df_train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
# using backwards Step Selection
glm_step = step(glm1, direction = "backward") # stepwise backward elim.
summary(glm_step)
glmf <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_train)
summary(glmf)
vif(glmf) # no colinearity detected
# Predict the probability on TEST - cutoff 0.5
df_test$PredProb <- predict.glm(glmf, newdata = df_test, type = 'response')
#create Prediction Indicators for y
df_test$Pred_Y <- ifelse(df_test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 74%
# sens: 80%
# Spec: 70%
plot(glmf)
##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY
# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#### end of default cutoff of 0.5
#### optimal cutoff
#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int)
pred
#Predicted Probability and True Classification
# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc
#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))
# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values),
type="l", lwd=2,
ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))
par(new=TRUE) # plot another line in same plot
#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values),
type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')
#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity
abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)
##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%
## best sensitivity
df_test$Pred_Y_sens <- ifelse(df_test$PredProb >= 0.3, 1, 0)
caret::confusionMatrix(as.factor(df_test$Pred_Y_sens), as.factor(df_test$y_int), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 60%
# sens: 85%
# Spec: 34%
# creating binary numeric variable for linear model
str(df2)
# first exploring with backwards step selection on lm with full data
lm1 <- lm(y_int ~ . -y, data = df_train)
summary(lm1)
lm1_step <- step(lm1, direction = "backward")
summary(lm1_step)
# final lm model base off of step
lmf <- lm(y_int ~ age + contact + campaign + poutcome + emp.var.rate +
cons.price.idx + cons.conf.idx, data = df_train)
#summary(lmf)
vif(lmf) # confirm there is no co linearity
# R- Squared is very small at 27% but is expectedc since y is binomial
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(y_int ~ . -y, data = df_train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
# using backwards Step Selection
glm_step = step(glm1, direction = "backward") # stepwise backward elim.
summary(glm_step)
glmf <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_train)
summary(glmf)
vif(glmf) # no colinearity detected
# Predict the probability on TEST - cutoff 0.5
df_test$PredProb <- predict.glm(glmf, newdata = df_test, type = 'response')
#create Prediction Indicators for y
df_test$Pred_Y <- ifelse(df_test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 74%
# sens: 80%
# Spec: 70%
plot(glmf)
##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY
# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#### end of default cutoff of 0.5
#### optimal cutoff
#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int)
pred
#Predicted Probability and True Classification
# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc
#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))
# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values),
type="l", lwd=2,
ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))
par(new=TRUE) # plot another line in same plot
#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values),
type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')
#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity
abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)
##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%
## best sensitivity
df_test$Pred_Y_sens <- ifelse(df_test$PredProb >= 0.3, 1, 0)
caret::confusionMatrix(as.factor(df_test$Pred_Y_sens), as.factor(df_test$y_int), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 60%
# sens: 85%
# Spec: 34%
#| echo: false
library(caret)
library(here)
library(lattice)
library(ggplot2)
library(logistf)
library(MASS)
library(tidyverse)
library(corrplot)
library(car)
library(ROCR)
library(dplyr)
#| echo: false
getwd() #open R project file to have the same file path
df1 = read.csv2('Data/bank-additional.csv') # Reading the csv from the location above
#| echo: false
str(df1) #probably remove from final paper
#| echo: false
#change y into factor
df1$y<-as.factor(df1$y)
df1$job<- as.factor(df1$job)
df1$marital <- as.factor(df1$marital)
df1$education <- as.factor(df1$education)
df1$default <- as.factor(df1$default)
df1$housing<- as.factor(df1$housing)
df1$loan <- as.factor(df1$loan)
df1$contact <- as.factor(df1$contact)
df1$month <- as.factor(df1$month)
df1$day_of_week <- as.factor(df1$day_of_week)
df1$poutcome <- as.factor(df1$poutcome)
df1$emp.var.rate <- as.numeric(df1$emp.var.rate)
df1$cons.price.idx <- as.numeric(df1$cons.price.idx)
df1$cons.conf.idx <- as.numeric(df1$cons.conf.idx)
df1$euribor3m <- as.numeric(df1$euribor3m)
df1$nr.employed <- as.integer(df1$nr.employed)
unique(df1$previous)
unique(df1$pdays)
df1$y_int <- ifelse(df1$y == 'yes', 1, 0)
df1$ppdays_ind <- ifelse(df1$pdays == 999, 0, 1)
df1$married_ind <- ifelse(df1$marital == 'married', 1, 0)
#Pdays - make it an indicator of not/yes
#duration - amt of time you have them, so same as Y
#| echo: false
#data is unbalanced 3.6k no, 451 yes
summary(df1$y)
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long = df1 %>%
dplyr::select(age, campaign, previous, emp.var.rate, cons.price.idx,
cons.conf.idx, euribor3m, nr.employed) %>%
pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")
#facet view
ggplot(bank_long, aes(x = Value)) +
geom_histogram(bins = 30, fill = "gray", color = "black") +
facet_wrap(~ Variable, scales = "free") +
theme_minimal() +
labs(title = "Distribution of Variables", x = NULL, y = "Frequency") +
theme(strip.text = element_text(face = "bold"))
#| echo: false
#testing variables
##we can discuss this ones during class
bank_long_cat = df1 %>%
dplyr::select(job, marital, education, default, housing, loan, contact, poutcome) %>%
pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")
#facet view
ggplot(bank_long_cat, aes(x = Value)) +
geom_bar(fill = "gray", color = "black") +
facet_wrap(~ Variable, scales = "free", ncol = 3) +
theme_minimal() +
labs(title = "Distribution of Categorical Variables", x = NULL, y = "Count") +
theme(strip.text = element_text(face = "bold"), axis.text.x = element_text(angle = 45, hjust = 1))
#| echo: false
#keep variables I want for Modeling
df2 <- df1 %>% select(age,job, married_ind, education, default, housing, loan, contact, campaign,
ppdays_ind, previous, poutcome, emp.var.rate, cons.price.idx, cons.conf.idx,
euribor3m, nr.employed, y, y_int)
#| echo: false
# since data is unbalanced we should take a more balanced sample
set.seed(42)
df_y_y = df2 %>% filter(y == "yes")
df_y_n = df2 %>% filter(y == "no")
sample_y = sample_n(df_y_n, nrow(df_y_y))
df_bal = rbind(df_y_y,sample_y)
#| echo: false
# test and train split
df_bal_split <- sample(nrow(df_bal),0.8*nrow(df_bal),replace = F) # Setting training sample to be 80% of the data
df_train <- df_bal[df_bal_split,]
df_test <- df_bal[-df_bal_split,]
#| echo: false
# creating binary numeric variable for linear model
str(df2)
# first exploring with backwards step selection on lm with full data
lm1 <- lm(y_int ~ . -y, data = df_train)
summary(lm1)
lm1_step <- step(lm1, direction = "backward")
summary(lm1_step)
# final lm model base off of step
lmf <- lm(y_int ~ age + contact + campaign + poutcome + emp.var.rate +
cons.price.idx + cons.conf.idx, data = df_train)
#summary(lmf)
vif(lmf) # confirm there is no co linearity
# R- Squared is very small at 27% but is expectedc since y is binomial
### LETS TRY SAME TECHNIQUE BUT WITH LOGISTIC MODEL
#glm model
glm1 <- glm(y_int ~ . -y, data = df_train, family = binomial) #Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
summary(glm1)
# using backwards Step Selection
glm_step = step(glm1, direction = "backward") # stepwise backward elim.
summary(glm_step)
glmf <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_train)
summary(glmf)
vif(glmf) # no colinearity detected
# Predict the probability on TEST - cutoff 0.5
df_test$PredProb <- predict.glm(glmf, newdata = df_test, type = 'response')
#create Prediction Indicators for y
df_test$Pred_Y <- ifelse(df_test$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 74%
# sens: 80%
# Spec: 70%
plot(glmf)
##DETER4MINING BEST THRESHOLD FOR BEST SENSITIVITY
# Predict the probability (p) of x23strat
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#### end of default cutoff of 0.5
#### optimal cutoff
#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int)
pred
#Predicted Probability and True Classification
# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc
#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))
# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values),
type="l", lwd=2,
ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))
par(new=TRUE) # plot another line in same plot
#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values),
type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')
#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity
abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)
##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%
## best sensitivity
df_test$Pred_Y_sens <- ifelse(df_test$PredProb >= 0.3, 1, 0)
caret::confusionMatrix(as.factor(df_test$Pred_Y_sens), as.factor(df_test$y_int), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 60%
# sens: 85%
# Spec: 34%
#| echo: false
# is there high correlation in any of our numeric variables
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
#looks like there is higher correlation between pdays and previous but it is under .6
corrplot(M, method = c("color"))
#| echo: false
summary(df1$y)
#| echo: false
summary(df_bal$y)
summary(lmf)
summary(glmf)
exp(coef(glmf)['age'])
exp(coef(glmf)['contacttelephone'])
exp(coef(glmf)['campaign            '])
exp(coef(glmf)['previous             '])
exp(coef(glmf)['poutcomenonexistent  '])
exp(coef(glmf)['poutcomesuccess      '])
exp(coef(glmf)['emp.var.rate'])
exp(coef(glmf)['cons.price.idx'])
exp(coef(glmf)['cons.conf.idx'])
# Predict the probability (p) of
glmftest <- glm(formula = y_int ~ age + contact + campaign + previous + poutcome +
emp.var.rate + cons.price.idx + cons.conf.idx, family = binomial,
data = df_test)
probabilities <- predict(glmftest, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#### end of default cutoff of 0.5
#### optimal cutoff
#ROC Curve and AUC
pred <- prediction(probabilities,df_test$y_int)
pred
#Predicted Probability and True Classification
# area under curve
auc <- round(as.numeric(performance(pred, measure = "auc")@y.values),3)
auc
#plotting the ROC curve and computing AUC
perf <- performance(pred, "tpr","fpr")
plot(perf,colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc))
# computing threshold for cutoff to best trade off sensitivity and specificity
#first sensitivity
plot(unlist(performance(pred, "sens")@x.values), unlist(performance(pred, "sens")@y.values),
type="l", lwd=2,
ylab="Sensitivity", xlab="Cutoff", main = paste("Maximized Cutoff\n","AUC: ",auc))
par(new=TRUE) # plot another line in same plot
#second specificity
plot(unlist(performance(pred, "spec")@x.values), unlist(performance(pred, "spec")@y.values),
type="l", lwd=2, col='red', ylab="", xlab="")
axis(4, at=seq(0,1,0.2)) #specificity axis labels
mtext("Specificity",side=4, col='red')
#find where the lines intersect
min.diff <-which.min(abs(unlist(performance(pred, "sens")@y.values) - unlist(performance(pred, "spec")@y.values)))
min.x<-unlist(performance(pred, "sens")@x.values)[min.diff]
min.y<-unlist(performance(pred, "spec")@y.values)[min.diff]
optimal <-min.x #this is the optimal points to best trade off sensitivity and specificity
abline(h = min.y, lty = 3)
abline(v = min.x, lty = 3)
text(min.x,0,paste("optimal threshold=",round(optimal,2)), pos = 3)
##OPTIMAL CUTOFF FOR BEST SENSITIVITY and specificity
#create Prediction Indicators for y
df_test$Pred_Y_best <- ifelse(df_test$PredProb >= 0.46, 1, 0)
caret::confusionMatrix(as.factor(df_test$y_int),as.factor(df_test$Pred_Y_best), positive = '1') #this function and package auto computes a lot of the metrics
# accuracy: 73%
# sens: 77%
# Spec: 70%
#| echo: false
exp(coef(glmf)['age'])
exp(coef(glmf)['contacttelephone'])
exp(coef(glmf)['campaign            '])
exp(coef(glmf)['previous             '])
exp(coef(glmf)['poutcomenonexistent  '])
exp(coef(glmf)['poutcomesuccess      '])
exp(coef(glmf)['emp.var.rate'])
exp(coef(glmf)['cons.price.idx'])
exp(coef(glmf)['cons.conf.idx'])
#| echo: false
par(mfrow=c(1,1), mar=c(1, 1, 1, 1), pin=c(4, 4))  # Adjust pin to control plot size
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
corrplot(M, method = "color")
#| echo: false
par(mfrow=c(1,1), mar=c(1, 1, 1, 1), pin=c(3, 3))  # Adjust pin to control plot size
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
corrplot(M, method = "color")
#| echo: false
par(mfrow=c(1,1), mar=c(.5, .5, .5, .5), pin=c(3, 3))  # Adjust pin to control plot size
df_num <- dplyr::select_if(df1, is.numeric)
M = cor(df_num)
corrplot(M, method = "color")
dow_data = read.csv('Data/dow_jones_index.data') # change variable name to match code
View(dow_data)
dow_names = read.csv('Data/dow_jones_index.names')  # change variable name to match code
View(dow_names)
library(dplyr)
library(ggplot2)
library(tree)
library(dplyr)
library(ggplot2)
intsall.library("tree")
library(dplyr)
library(ggplot2)
install.packages("tree")
library(tree)
library(ISLR)
#getwd() #open R project file to have the same file path
dow_data = read.csv('Data/dow_jones_index.data') # change variable name to match code
#dow_names = read.csv('Data/dow_jones_index.names')  # change variable name to match code
## Taking the $ sign off
dow_data <- dow_data %>%
mutate(open = as.numeric(gsub("\\$", "", open)),
high = as.numeric(gsub("\\$", "", high)),
low = as.numeric(gsub("\\$", "", low)),
next_weeks_open = as.numeric(gsub("\\$", "", next_weeks_open)),
next_weeks_close = as.numeric(gsub("\\$", "", next_weeks_close)),
close = as.numeric(gsub("\\$", "", close)))
## remove missing values
dow_data <- na.omit(dow_data)
dow_data$Positive_percent_change_ind <- factor(ifelse(dow_data$percent_change_next_weeks_price > 0, "1", "0"))
## Splitting data based on PDF
train = dow_data %>%
filter(quarter == 1) %>%
select(-percent_change_next_weeks_price, -quarter, -date)
test = dow_data %>%
filter(quarter == 2) %>%
select(-percent_change_next_weeks_price, -quarter, -date)
## grow tree
Stock_tree <- tree(Positive_percent_change_ind ~ ., data = train)
## default output
Stock_tree
## result summary
summary(Stock_tree)
## plot tree
plot(Stock_tree)
text(Stock_tree, cex = 0.55)
# Evaluating
pred <- predict(Stock_tree, newdata = test, type = "class")
# accuarcy
table(pred, test$Positive_percent_change_ind)
set.seed(11)
cv_tree <- cv.tree(Stock_tree, FUN = prune.misclass)
cv_tree
# plot the estimated error from cross-validation
par(mfrow = c(1,2))
plot(cv_tree$size, cv_tree$dev, type = "b")
plot(cv_tree$k, cv_tree$dev, type = "b")
# best tree with smallest k (small prune, and tree size is large)
best_size <- cv_tree$size[which.min(cv_tree$dev)]
# building prune model
prune_tree <- prune.misclass(Stock_tree, best = best_size)
# plot prune tree
plot(prune_tree)
text(prune_tree, cex = .55)
# get predictions
pred_prune <- predict(prune_tree, newdata = test, type = "class")
#check accuracy
table(pred_prune, test$Positive_percent_change_ind)
summary(dow_data)
str(dow_data)
colSums(is.na(dow_data))
unique(dow_data$stock)
#{{< pagebreak >}}
